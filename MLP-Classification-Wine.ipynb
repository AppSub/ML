{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PACKAGES REQUIRED\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FETCHING DATA\n",
    "df = pd.read_csv(\"wine_dataset.csv\")\n",
    "df.shape\n",
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>NonFlavanoid Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Alcohol  Malic Acid   Ash  Alcalinity of ash  Magnesium  Total Phenols  \\\n",
       "0      14.23        1.71  2.43               15.6        127           2.80   \n",
       "1      13.20        1.78  2.14               11.2        100           2.65   \n",
       "2      13.16        2.36  2.67               18.6        101           2.80   \n",
       "3      14.37        1.95  2.50               16.8        113           3.85   \n",
       "4      13.24        2.59  2.87               21.0        118           2.80   \n",
       "..       ...         ...   ...                ...        ...            ...   \n",
       "173    13.71        5.65  2.45               20.5         95           1.68   \n",
       "174    13.40        3.91  2.48               23.0        102           1.80   \n",
       "175    13.27        4.28  2.26               20.0        120           1.59   \n",
       "176    13.17        2.59  2.37               20.0        120           1.65   \n",
       "177    14.13        4.10  2.74               24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  NonFlavanoid Phenols  Proanthocyanins  Color intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     OD280/OD315  Proline  Target  \n",
       "0           3.92     1065       0  \n",
       "1           3.40     1050       0  \n",
       "2           3.17     1185       0  \n",
       "3           3.45     1480       0  \n",
       "4           2.93      735       0  \n",
       "..           ...      ...     ...  \n",
       "173         1.74      740       2  \n",
       "174         1.56      750       2  \n",
       "175         1.56      835       2  \n",
       "176         1.62      840       2  \n",
       "177         1.60      560       2  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2] (178,)\n"
     ]
    }
   ],
   "source": [
    "outp = df['Target'].values\n",
    "print(outp,outp.shape)\n",
    "df = df.drop(['Target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "inp = scaler.fit_transform(df)\n",
    "# inp = inp.to_numpy()\n",
    "inputs = inp\n",
    "\n",
    "inputs,outp = shuffle(inp,outp)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.27631579, 0.11660079, 0.5026738 , ..., 0.4796748 , 0.71062271,\n",
       "        0.24750357],\n",
       "       [0.56315789, 0.36561265, 0.54010695, ..., 0.09756098, 0.12820513,\n",
       "        0.40085592],\n",
       "       [0.5       , 0.40909091, 0.71657754, ..., 0.23577236, 0.38095238,\n",
       "        0.2296719 ],\n",
       "       ...,\n",
       "       [0.34473684, 0.33794466, 0.58823529, ..., 0.2601626 , 0.77289377,\n",
       "        0.11412268],\n",
       "       [0.65263158, 0.20948617, 0.68983957, ..., 0.50406504, 0.58608059,\n",
       "        0.58273894],\n",
       "       [0.1       , 0.        , 0.60962567, ..., 0.50406504, 0.38095238,\n",
       "        0.11126961]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# df = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = inputs[0:140,:]\n",
    "train_y = outp[0:140]\n",
    "test_X = inputs[141:,:]\n",
    "test_y = outp[141:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input neurons, hidden neurons, output neurons\n",
    "inp_layer = 13\n",
    "hidden_layer = 10\n",
    "outp_layer = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_mul_bias(A, B, bias): # Matrix multiplication (for Testing)\n",
    "    C = [[0 for i in range(len(B[0]))] for i in range(len(A))]    \n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B[0])):\n",
    "            for k in range(len(B)):\n",
    "                C[i][j] += A[i][k] * B[k][j]\n",
    "            C[i][j] += bias[j]\n",
    "    return C\n",
    "\n",
    "def vec_mat_bias(A, B, bias): # Vector (A) x matrix (B) multiplication\n",
    "    C = [0 for i in range(len(B[0]))]\n",
    "#     print('C:',C)\n",
    "    for j in range(len(B[0])):\n",
    "        for k in range(len(B)):\n",
    "            C[j] += A[k] * B[k][j]\n",
    "#             print(bias[j])\n",
    "            C[j] += bias[j]\n",
    "    return C\n",
    "\n",
    "\n",
    "def mat_vec(A, B): # Matrix (A) x vector (B) multipilicatoin (for backprop)\n",
    "    C = [0 for i in range(len(A))]\n",
    "    for i in range(len(A)):\n",
    "        for j in range(len(B)):\n",
    "            C[i] += A[i][j] * B[j]\n",
    "    return C\n",
    "\n",
    "def sigmoid(A, deriv=False):\n",
    "    if deriv: # derivation of sigmoid (for backprop)\n",
    "        for i in range(len(A)):\n",
    "            A[i] = A[i] * (1 - A[i])\n",
    "    else:\n",
    "        for i in range(len(A)):\n",
    "            A[i] = 1 / (1 + math.exp(-A[i]))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter\n",
    "learning_rate = 0.05\n",
    "epoch = 1000\n",
    "neuron = [13, 10, 3] # number of neuron each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHT1:  [[-0.7006427424204122, 0.029686827775752933, -0.8714856610746777, -0.35081936443336237, 0.7195236508144898, 0.5051447965759845, 0.8532743487523844, -0.07374098664688655, 0.8672775156941295, -0.3657047793512953], [0.02751793965767968, 0.9756160919838972, -0.6186610890984339, -0.6373146210148779, -0.2996442093942755, 0.14932022756860897, 0.1697193768532177, 0.6990651279955842, -0.42589141612584314, 0.14932635138620687], [-0.9275505357711527, 0.9748741897829343, 0.8984741606676145, 0.47964965007492455, 0.16200350262375052, 0.9359868457865761, 0.13059060307717707, -0.365242550426363, 0.8823473075049253, 0.5959649809571086], [-0.5729673328374665, -0.6208764063609773, 0.8961755544034824, -0.877205659033578, 0.9415674345055627, -0.2995504615890887, 0.19439014735556337, 0.30466419830986546, 0.8940256014631582, 0.008221648753744537], [-0.7350817549698829, -0.7219835241203623, 0.8736854775511012, 0.6586917315062204, -0.2726072446696943, 0.6445637094683145, 0.08870927654997796, 0.5236845966729002, -0.6127973036012784, -0.4593882951935091], [0.7197188648330799, -0.42952592593015826, 0.7972350905592287, 0.4355960288528118, 0.40974196761898507, 0.8689453875173831, 0.30747828890042883, 0.5880831995200269, -0.4545631993193424, -0.0871548640831985], [-0.8673238772843708, -0.37463878195857614, -0.5565658450303141, 0.669655769367244, -0.08346634774447259, 0.1635460278518781, 0.39770171151898137, 0.9265087006737298, 0.6479061069996015, 0.47171071486383465], [0.039399468619187195, 0.8169605905417838, -0.7990786122268061, -0.32133142355500643, -0.9081399861835706, 0.6467569494535028, -0.6975815872636693, -0.7264482142132065, 0.1770753598940389, -0.19522172357281997], [-0.03245656503977079, 0.9610364433055159, 0.8632530086025538, -0.27835592341652826, 0.22953077993286986, -0.8390368466120581, -0.11126372786055061, 0.7688282398374024, 0.7737512492670766, 0.823484433356986], [-0.7123643107654443, 0.8752746033829963, 0.32857500398736605, -0.006890290359653051, 0.10910728460715458, -0.5447552296793456, 0.7236739046428218, -0.9183937743251289, -0.482542206303451, 0.7160117003256359], [0.23875474345032277, -0.4047799920612445, 0.6027109228258714, 0.44395743809077537, 0.13653484413903105, -0.7480933481889593, 0.24631637223362812, 0.03664462488125375, -0.11533749530859749, -0.13795202945579144], [0.0186713582461151, 0.8623065784880346, -0.9212948078886936, 0.5776266746975003, -0.6496787054024951, -0.1659019685081229, -0.16516249554822138, -0.2739639149329709, -0.05738192800506825, -0.7982304830729374], [0.5153928805894037, -0.8801732528773025, -0.5605478435037292, -0.06510672272808127, -0.4389586967520691, -0.20802938150028405, 0.5579131926255245, 0.7409345139700019, 0.36935222334117457, 0.13495988174767737]]\n",
      "WEIGHT2:  [[-0.04104904068705428, -0.7128249465828849, 0.4439806949271283], [0.331345357535483, -0.6552687342716605, 0.4366055276095675], [0.2809656043261872, 0.11111617875782276, 0.2531785563900717], [-0.48314068117760645, 0.6578718338295957, 0.9725219493775707], [-0.5006475893677205, -0.8061729809472171, -0.9163862376045626], [0.6254884989151961, -0.17336732930481813, 0.057836915000096134], [0.7065107615437916, 0.7781937635269127, 0.644442986702086], [-0.24154289716615618, -0.9091462400315784, 0.6972722113036265], [-0.3797807342586992, 0.8038247382211876, 0.21087681714680517], [0.712194716327277, -0.18763854062821195, -0.38885249188907234]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# Initiate weight and bias with 0 value\n",
    "weight = [[0 for j in range(neuron[1])] for i in range(neuron[0])]\n",
    "weight_2 = [[0 for j in range(neuron[2])] for i in range(neuron[1])]\n",
    "bias = [0 for i in range(neuron[1])]\n",
    "bias_2 = [0 for i in range(neuron[2])]\n",
    "\n",
    "# Initiate weight with random between -1.0 ... 1.0\n",
    "for i in range(neuron[0]):\n",
    "    for j in range(neuron[1]):\n",
    "        weight[i][j] = 2 * random.random() - 1\n",
    "\n",
    "for i in range(neuron[1]):\n",
    "    for j in range(neuron[2]):\n",
    "        weight_2[i][j] = 2 * random.random() - 1\n",
    "        \n",
    "        \n",
    "print('WEIGHT1: ',weight)\n",
    "print('WEIGHT2: ',weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COST TOTAL FOR EVERY 100 ITERATIONS: \n",
      "0.25819286000496927\n",
      "0.050954066157575105\n",
      "0.0225857296531834\n",
      "0.015872166717286848\n",
      "0.01200677977761804\n",
      "0.009298136959458895\n",
      "0.0076980853020358385\n",
      "0.006796417693238368\n"
     ]
    }
   ],
   "source": [
    "print('COST TOTAL FOR EVERY 100 ITERATIONS: ')\n",
    "for e in range(epoch):\n",
    "    cost_total = 0\n",
    "    for idx, x in enumerate(train_X): # Update for each data; SGD\n",
    "        \n",
    "        # Forward propagation\n",
    "        h_1 = vec_mat_bias(x, weight, bias)\n",
    "        X_1 = sigmoid(h_1)\n",
    "        h_2 = vec_mat_bias(X_1, weight_2, bias_2)\n",
    "        X_2 = sigmoid(h_2)\n",
    "\n",
    "        # Convert to One-hot target\n",
    "#         print(e,idx)\n",
    "        target = [0, 0, 0]\n",
    "#         print(train_y[idx])\n",
    "        target[int(train_y[idx])] = 1\n",
    "\n",
    "        # Cost function, Square Root Error\n",
    "        eror = 0\n",
    "        for i in range(neuron[2]):\n",
    "            eror +=  (target[i] - X_2[i]) ** 2 \n",
    "        cost_total += eror * 1 / neuron[2]\n",
    "\n",
    "        # Backward propagation\n",
    "        # Update weight_2 and bias_2 (layer 2)\n",
    "        delta_2 = []\n",
    "        for j in range(neuron[2]):\n",
    "            delta_2.append(-1 * 2. / neuron[2] * (target[j]-X_2[j]) * X_2[j] * (1-X_2[j]))\n",
    "\n",
    "        for i in range(neuron[1]):\n",
    "            for j in range(neuron[2]):\n",
    "                weight_2[i][j] -= learning_rate * (delta_2[j] * X_1[i])\n",
    "                bias_2[j] -= learning_rate * delta_2[j]\n",
    "        \n",
    "        # Update weight and bias (layer 1)\n",
    "        delta_1 = mat_vec(weight_2, delta_2)\n",
    "        for j in range(neuron[1]):\n",
    "            delta_1[j] = delta_1[j] * (X_1[j] * (1-X_1[j]))\n",
    "        \n",
    "        for i in range(neuron[0]):\n",
    "            for j in range(neuron[1]):\n",
    "                weight[i][j] -=  learning_rate * (delta_1[j] * x[i])\n",
    "                bias[j] -= learning_rate * delta_1[j]\n",
    "    \n",
    "    cost_total /= len(train_X)\n",
    "    \n",
    "    if(e % 100 == 0):\n",
    "        print(cost_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('WEIGHT MATRICES AFTER LEARNING FROM THE MODEL:')\n",
    "print('WEIGHT 1: \\n',weight)\n",
    "print('\\nWEIGHT 2: \\n',weight_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = matrix_mul_bias(test_X, weight, bias)\n",
    "res_2 = matrix_mul_bias(res, weight_2, bias)\n",
    "\n",
    "# Get prediction\n",
    "preds = []\n",
    "for r in res_2:\n",
    "    preds.append(max(enumerate(r), key=lambda x:x[1])[0])\n",
    "\n",
    "# Print prediction\n",
    "print('Pred:' , 'Actual:')\n",
    "for i in range(0,20):\n",
    "    print(preds[i],'\\t',int(test_y[i]))\n",
    "\n",
    "# Calculate accuration\n",
    "acc = 0.0\n",
    "for i in range(len(preds)):\n",
    "    if preds[i] == int(test_y[i]):\n",
    "        acc += 1\n",
    "print('ACCURACY: ',acc / len(preds) * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
